{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste da taxa de aprendizado e número de iterações\n",
    "\n",
    "A primeira etada é ajustar a taxa de aprendizado e o número de iterações. Após alterar os outros hyperparameters (fator de regularização e grau dos atributos polinimiais) será necessário realizar o ajuste fino, mas será algum valor próximo ao encontrado nesta etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** imports necessários: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from regressao_logistica_regularizado import RegularizedLogisticRegression\n",
    "from experimentos import Dataset as DATASET\n",
    "from experimentos import ModelSelection\n",
    "from experimentos import Plot\n",
    "from dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** varia alpha e número de iterações e plota os gráficos: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "#degree = 18\n",
    "\n",
    "#X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "#X_ =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "X_ =  DTS.dataset_scaling(X)\n",
    "\n",
    "alphas = [14, 10, 7]\n",
    "epochs = e\n",
    "Lambda = 0\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "fold = 1\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "for train,val in MS.k_fold(X_, k=5, shuffle=True):\n",
    "    fig, ax = plt.subplots()\n",
    "    errors[str(fold)] = []\n",
    "    val_errors[str(fold)] = []\n",
    "    loss[str(fold)] = []\n",
    "    legends = []\n",
    "    for i,a in enumerate(alphas):\n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y[train], X_[val], Y[val], epochs=e, learning_rate=a, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error)\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error)\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "        ax.plot(range(e+1), RLR.loss, color=colors[i], linewidth=1)\n",
    "        legends.append(mpatches.Patch(color=colors[i], label='alpha = ' + str(a)))\n",
    "        plt.ylim([0.1, 1])\n",
    "        plt.xlabel(u'Épocas')\n",
    "        plt.ylabel(u'Custo')\n",
    "        \n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "        box.width, box.height * 0.9])\n",
    "        ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "        plt.grid(True)\n",
    "    \n",
    "        fig.savefig('fold' + str(fold) + '.eps')\n",
    "        #plt.show()\n",
    "        plt.close(fig)\n",
    "        \n",
    "        fold += 1\n",
    "        \n",
    "error = []        \n",
    "val_error = []\n",
    "loss_ = []\n",
    "for k in ['1', '2', '3', '4','5']:\n",
    "    error.append(errors[k])\n",
    "    val_error.append(val_errors[k])\n",
    "    loss_.append(loss[k])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i,a in enumerate(alphas):\n",
    "    mean = np.asarray(loss_).mean(axis=0)[i]\n",
    "    stdeviation = np.asarray(loss_).std(axis=0)[i]\n",
    "    print a, mean[-1], stdeviation[-1]\n",
    "    \n",
    "    ax.plot(range(epochs[0]+1), mean, color=colors[i], linewidth=1)\n",
    "    ax.fill_between(range(epochs[0]+1), mean-stdeviation, mean+stdeviation , color=colors[i], linewidth=1, alpha=0.2)\n",
    "    \n",
    "plt.ylim([0.1, 1.])\n",
    "plt.xlabel(u'Épocas')\n",
    "plt.ylabel(u'Custo')\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('mnist_eta_alpha_fino.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de validação: atributos polinomiais\n",
    "\n",
    "O objetivo deste experimento é analisar se a geração de atributos polinomiais trazem melhoria de\n",
    "desempenho e, mais especificamente, qual o grau de polinômio mais adequado para a tarefa. Portanto,\n",
    "deve-se variar o grau dos atributos polinomiais a partir de 1 (sem atributos polinomiais) até um valor que\n",
    "a curva de validação indique claramente que há overfitting. Neste experimento, não utilize\n",
    "regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "# normaliza\n",
    "#X_ =  DTS.dataset_scaling(X)\n",
    "\n",
    "lr = 10\n",
    "degrees = range(1,31)\n",
    "e = 1000\n",
    "Lambda = 0\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "fold = 1\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "for train,val in MS.k_fold(X, k=5, shuffle=True):\n",
    "    errors[str(fold)] = []\n",
    "    val_errors[str(fold)] = []\n",
    "    loss[str(fold)] = []\n",
    "    \n",
    "    for d in degrees:\n",
    "        X_pol = DTS.generate_polynomial_attributes(X, d)\n",
    "        # normaliza\n",
    "        X_ =  DTS.dataset_scaling(X_pol)\n",
    "        \n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y[train], X_[val], Y[val], epochs=e, learning_rate=lr, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "    fold += 1\n",
    "        \n",
    "train_error = []        \n",
    "val_error = []\n",
    "loss_ = []\n",
    "for k in ['1', '2', '3', '4','5']:\n",
    "    train_error.append(errors[k])\n",
    "    val_error.append(val_errors[k])\n",
    "    loss_.append(loss[k])\n",
    "\n",
    "train_error = np.array(train_error)\n",
    "val_error = np.array(val_error)\n",
    "    \n",
    "train_mean, train_std = train_error.mean(axis=0), train_error.std(axis=0)\n",
    "\n",
    "val_mean, val_std = val_error.mean(axis=0), val_error.std(axis=0)\n",
    "\n",
    "print train_mean\n",
    "print train_std\n",
    "print\n",
    "print val_mean\n",
    "print val_std\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(degrees, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(degrees, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(degrees, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(degrees, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "plt.xlabel('$\\eta$')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(degrees)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_eta.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Curva de validação: Regularização\n",
    "\n",
    "Neste experimento, o impacto do termo de regularização da descida de gradiente será\n",
    "investigado. O objetivo é utilizar a geração de atributos polinomiais de um determinado grau onde\n",
    "ocorra, claramente, uma situação de overfitting. Através da variação do peso da regularização, deve-se\n",
    "investigar qual o valor ideal deste parâmetro. Isto deve ser feito através da análise da curva de\n",
    "validação para este parâmetro. O objetivo final deste experimento é verificar se a utilização de\n",
    "regularização de um modelo complexo (que apresenta forte overfitting sem o uso da regularização)\n",
    "apresenta desempenho melhor do que a calibração do grau dos atributos polinomiais (experimento\n",
    "anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 18\n",
    "\n",
    "X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "X_ =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "#X_ =  DTS.dataset_scaling(X)\n",
    "\n",
    "lr = 10\n",
    "e = 1000\n",
    "Lambda = [0, 0.0005, 0.001, 0.005]\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "fold = 1\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "for train,val in MS.k_fold(X, k=5, shuffle=True):\n",
    "    errors[str(fold)] = []\n",
    "    val_errors[str(fold)] = []\n",
    "    loss[str(fold)] = []\n",
    "    \n",
    "    for l in Lambda:    \n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y[train], X_[val], Y[val], epochs=e, learning_rate=lr, Lambda=l, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "    fold += 1\n",
    "        \n",
    "train_error = []        \n",
    "val_error = []\n",
    "loss_ = []\n",
    "for k in ['1', '2', '3', '4','5']:\n",
    "    train_error.append(errors[k])\n",
    "    val_error.append(val_errors[k])\n",
    "    loss_.append(loss[k])\n",
    "\n",
    "train_error = np.array(train_error)\n",
    "val_error = np.array(val_error)\n",
    "    \n",
    "train_mean, train_std = train_error.mean(axis=0), train_error.std(axis=0)\n",
    "\n",
    "val_mean, val_std = val_error.mean(axis=0), val_error.std(axis=0)\n",
    "\n",
    "print train_mean\n",
    "print train_std\n",
    "print\n",
    "print val_mean\n",
    "print val_std\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Lambda, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(Lambda, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(Lambda, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(Lambda, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(Lambda)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_lambda.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de aprendizado\n",
    "\n",
    "Neste experimento, deve-se utilizar a curva de aprendizado para analisar o melhor modelo\n",
    "encontrado nos experimentos anteriores (grau de polinômio e peso da regularização). A curva gerada\n",
    "deve ser analisada para depurar o modelo e concluir quais atitudes podem ser tomadas para melhorar o\n",
    "desempenho do classificador, se for necessário. A curva gerada deve conter, pelo menos, 5 pontos (5\n",
    "tamanhos diferentes do conjunto de treino)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** atributos polinomiais **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 2\n",
    "\n",
    "#X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "#X_norm =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "X_norm =  DTS.dataset_scaling(X)\n",
    "\n",
    "size = X.shape[0]\n",
    "\n",
    "M = [int(0.6*size), int(0.7*size), int(0.8*size), int(0.9*size), int(0.95*size), int(size)]\n",
    "\n",
    "lr = 10\n",
    "e = 1000\n",
    "Lambda = 0\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "indices = range(size)\n",
    "\n",
    "train_mean = []\n",
    "train_std = []\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "for m in M:\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = np.array(X_norm[indices[0:m]])\n",
    "    Y_ = np.array(Y[indices[0:m]])\n",
    "    fold = 1\n",
    "    for train,val in MS.k_fold(X_, k=5, shuffle=True):\n",
    "        errors[str(fold)] = []\n",
    "        val_errors[str(fold)] = []\n",
    "        loss[str(fold)] = []\n",
    "\n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y_[train], X_[val], Y_[val], epochs=e, learning_rate=lr, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "        fold += 1\n",
    "        \n",
    "    train_error = []        \n",
    "    val_error = []\n",
    "    loss_ = []\n",
    "    for k in ['1', '2', '3', '4','5']:\n",
    "        train_error.append(errors[k])\n",
    "        val_error.append(val_errors[k])\n",
    "        loss_.append(loss[k])\n",
    "\n",
    "    train_error = np.array(train_error)\n",
    "    val_error = np.array(val_error)\n",
    "\n",
    "    train_mean.append(train_error.mean(axis=0)[0])\n",
    "    train_std.append(train_error.std(axis=0)[0])\n",
    "\n",
    "    val_mean.append(val_error.mean(axis=0)[0])\n",
    "    val_std.append(val_error.std(axis=0)[0])\n",
    "\n",
    "train_mean = np.array(train_mean)\n",
    "train_std = np.array(train_std)\n",
    "val_mean = np.array(val_mean)\n",
    "val_std = np.array(val_std)\n",
    "\n",
    "print train_mean\n",
    "print train_std\n",
    "print\n",
    "print val_mean\n",
    "print val_std\n",
    "print\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(M, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(M, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(M, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(M, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "plt.xlabel('M')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(M)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_M_pol.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** regularização **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 18\n",
    "\n",
    "X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "X_norm =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "#X_norm =  DTS.dataset_scaling(X)\n",
    "\n",
    "size = X.shape[0]\n",
    "\n",
    "M = [int(0.6*size), int(0.7*size), int(0.8*size), int(0.9*size), int(0.95*size), int(size)]\n",
    "\n",
    "lr = 10\n",
    "e = 1000\n",
    "Lambda = 0.001\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "indices = range(size)\n",
    "\n",
    "train_mean = []\n",
    "train_std = []\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "for m in M:\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = np.array(X_norm[indices[0:m]])\n",
    "    Y_ = np.array(Y[indices[0:m]])\n",
    "    fold = 1\n",
    "    for train,val in MS.k_fold(X_, k=5, shuffle=True):\n",
    "        errors[str(fold)] = []\n",
    "        val_errors[str(fold)] = []\n",
    "        loss[str(fold)] = []\n",
    "\n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y_[train], X_[val], Y_[val], epochs=e, learning_rate=lr, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "        fold += 1\n",
    "        \n",
    "    train_error = []        \n",
    "    val_error = []\n",
    "    loss_ = []\n",
    "    for k in ['1', '2', '3', '4','5']:\n",
    "        train_error.append(errors[k])\n",
    "        val_error.append(val_errors[k])\n",
    "        loss_.append(loss[k])\n",
    "\n",
    "    train_error = np.array(train_error)\n",
    "    val_error = np.array(val_error)\n",
    "\n",
    "    train_mean.append(train_error.mean(axis=0)[0])\n",
    "    train_std.append(train_error.std(axis=0)[0])\n",
    "\n",
    "    val_mean.append(val_error.mean(axis=0)[0])\n",
    "    val_std.append(val_error.std(axis=0)[0])\n",
    "\n",
    "train_mean = np.array(train_mean)\n",
    "train_std = np.array(train_std)\n",
    "val_mean = np.array(val_mean)\n",
    "val_std = np.array(val_std)\n",
    "  \n",
    "print train_mean\n",
    "print train_std\n",
    "print\n",
    "print val_mean\n",
    "print val_std\n",
    "print\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(M, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(M, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(M, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(M, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "plt.xlabel('M')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(M)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_M_reg.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
