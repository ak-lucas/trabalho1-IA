{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste da taxa de aprendizado e número de iterações\n",
    "\n",
    "A primeira etada é ajustar a taxa de aprendizado e o número de iterações. Após alterar os outros hyperparameters (fator de regularização e grau dos atributos polinimiais) será necessário realizar o ajuste fino, mas será algum valor próximo ao encontrado nesta etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** imports necessários: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from regressao_logistica_regularizado import RegularizedLogisticRegression\n",
    "from experimentos import Dataset as DATASET\n",
    "from experimentos import ModelSelection\n",
    "from experimentos import Plot\n",
    "from dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** varia alpha e número de iterações e plota os gráficos: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 18\n",
    "\n",
    "X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "X_ =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "#X_ =  DTS.dataset_scaling(X)\n",
    "\n",
    "alphas = [14, 10, 7]\n",
    "epochs = [1000]\n",
    "Lambda = 0\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "fold = 1\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "for train,val in MS.k_fold(X_, k=5, shuffle=True):\n",
    "    fig, ax = plt.subplots()\n",
    "    errors[str(fold)] = []\n",
    "    val_errors[str(fold)] = []\n",
    "    loss[str(fold)] = []\n",
    "    for e in epochs:\n",
    "        legends = []\n",
    "        for i,a in enumerate(alphas):\n",
    "            # ajusta o modelo\n",
    "            RLR.fit(X_[train], Y[train], X_[val], Y[val], epochs=e, learning_rate=a, Lambda=Lambda, print_results=False)\n",
    "            \n",
    "            # calcula o erro no treino\n",
    "            errors[str(fold)].append(RLR.train_error)\n",
    "            # calcula o erro no teste\n",
    "            val_errors[str(fold)].append(RLR.val_error)\n",
    "            # salva loss\n",
    "            loss[str(fold)].append(RLR.loss)\n",
    "            \n",
    "            ax.plot(range(e+1), RLR.loss, color=colors[i], linewidth=1)\n",
    "            legends.append(mpatches.Patch(color=colors[i], label='alpha = ' + str(a)))\n",
    "        #title = u'Ajuste da taxa com #epocas = ' + str(e) + ' e $\\lambda$ = ' + str(Lambda) + ' (fold ' + str(fold) + ')'\n",
    "        plt.ylim([0.1, 1])\n",
    "        plt.xlabel(u'Épocas')\n",
    "        plt.ylabel(u'Custo')\n",
    "        \n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "        box.width, box.height * 0.9])\n",
    "        ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "        #plt.title(title)\n",
    "        plt.grid(True)\n",
    "    \n",
    "        fig.savefig('fold' + str(fold) + '.eps')\n",
    "        #plt.show()\n",
    "        plt.close(fig)\n",
    "        \n",
    "        fold += 1\n",
    "        \n",
    "error = []        \n",
    "val_error = []\n",
    "loss_ = []\n",
    "for k in ['1', '2', '3', '4','5']:\n",
    "    error.append(errors[k])\n",
    "    val_error.append(val_errors[k])\n",
    "    loss_.append(loss[k])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i,a in enumerate(alphas):\n",
    "    mean = np.asarray(loss_).mean(axis=0)[i]\n",
    "    stdeviation = np.asarray(loss_).std(axis=0)[i]\n",
    "    print a, mean[-1], stdeviation[-1]\n",
    "    \n",
    "    ax.plot(range(epochs[0]+1), mean, color=colors[i], linewidth=1)\n",
    "    ax.fill_between(range(epochs[0]+1), mean-stdeviation, mean+stdeviation , color=colors[i], linewidth=1, alpha=0.2)\n",
    "    \n",
    "title = u'Curva de aprendizado média com #epocas = ' + str(e) + ' e $\\lambda$ = ' + str(Lambda)\n",
    "plt.ylim([0.1, 1.])\n",
    "plt.xlabel(u'Épocas')\n",
    "plt.ylabel(u'Custo')\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "#plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('mnist_eta_alpha_fino.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** erro por época ** (pode ignorar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(alphas)):\n",
    "    plt.plot(range(epochs[0]+1), np.asarray(error).mean(axis=0)[i], color=colors[i], linewidth=1)\n",
    "    #legends.append(mpatches.Patch(color=colors[i], label='alpha = ' + str(a)))\n",
    "title = 'erro no treino'\n",
    "plt.ylim([0.05, 0.6])\n",
    "plt.xlabel('epocas')\n",
    "plt.ylabel('erro')\n",
    "plt.legend(handles=legends[:], loc='upper right')\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('erro_treino.eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(alphas)):\n",
    "    plt.plot(range(epochs[0]+1), np.asarray(val_error).mean(axis=0)[i], color=colors[i], linewidth=1)\n",
    "    #legends.append(mpatches.Patch(color=colors[i], label='alpha = ' + str(a)))\n",
    "title = 'erro na validacao'\n",
    "plt.ylim([0.05, 0.6])\n",
    "plt.xlabel('epocas')\n",
    "plt.ylabel('erro')\n",
    "plt.legend(handles=legends[:], loc='upper right')\n",
    "plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('erro_validacao.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cada linha é um alpha, cada coluna uma época e cada valor é um erro binário médio dos k-folds\n",
    "#print \"erro medio no treino \", np.asarray(error).mean(axis=0).shape\n",
    "#print \"desvio padrao no treino \", np.asarray(error).std(axis=0)\n",
    "#print \"erro medio na validacao \", np.asarray(val_error).mean(axis=0).shape\n",
    "#print \"desvio padrao na validacao \", np.asarray(val_error).std(axis=0)\n",
    "\n",
    "for i,a in enumerate(alphas):\n",
    "    print \"alpha = \", a\n",
    "    print \"erro medio final no treino \", np.asarray(error).mean(axis=0)[i][-1]\n",
    "    print \"desvio padrao final no treino \", np.asarray(error).std(axis=0)[i][-1]\n",
    "    print \"erro medio final na validacao \", np.asarray(val_error).mean(axis=0)[i][-1]\n",
    "    print \"desvio padrao final na validacao \", np.asarray(val_error).std(axis=0)[i][-1]\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de validação: atributos polinomiais\n",
    "\n",
    "O objetivo deste experimento é analisar se a geração de atributos polinomiais trazem melhoria de\n",
    "desempenho e, mais especificamente, qual o grau de polinômio mais adequado para a tarefa. Portanto,\n",
    "deve-se variar o grau dos atributos polinomiais a partir de 1 (sem atributos polinomiais) até um valor que\n",
    "a curva de validação indique claramente que há overfitting. Neste experimento, não utilize\n",
    "regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "# normaliza\n",
    "#X_ =  DTS.dataset_scaling(X)\n",
    "\n",
    "lr = 10\n",
    "degrees = range(1,31)\n",
    "e = 1000\n",
    "Lambda = 0\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "fold = 1\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "for train,val in MS.k_fold(X, k=5, shuffle=True):\n",
    "    errors[str(fold)] = []\n",
    "    val_errors[str(fold)] = []\n",
    "    loss[str(fold)] = []\n",
    "    \n",
    "    for d in degrees:\n",
    "        X_pol = DTS.generate_polynomial_attributes(X, d)\n",
    "        # normaliza\n",
    "        X_ =  DTS.dataset_scaling(X_pol)\n",
    "        \n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y[train], X_[val], Y[val], epochs=e, learning_rate=lr, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "    fold += 1\n",
    "        \n",
    "train_error = []        \n",
    "val_error = []\n",
    "loss_ = []\n",
    "for k in ['1', '2', '3', '4','5']:\n",
    "    train_error.append(errors[k])\n",
    "    val_error.append(val_errors[k])\n",
    "    loss_.append(loss[k])\n",
    "\n",
    "train_error = np.array(train_error)\n",
    "val_error = np.array(val_error)\n",
    "    \n",
    "train_mean, train_std = train_error.mean(axis=0), train_error.std(axis=0)\n",
    "\n",
    "val_mean, val_std = val_error.mean(axis=0), val_error.std(axis=0)\n",
    "\n",
    "print train_mean\n",
    "print train_std\n",
    "print\n",
    "print val_mean\n",
    "print val_std\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(degrees, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(degrees, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(degrees, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(degrees, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "#ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "title = u'Curva de aprendizado média com #epocas = ' + str(e) + ', $\\lambda$ = ' + str(Lambda) + ' e $\\alpha$ = ' + str(lr)\n",
    "#plt.ylim([0.1, 1.])\n",
    "plt.xlabel('$\\eta$')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(degrees)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "#plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_eta.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Curva de validação: Regularização\n",
    "\n",
    "Neste experimento, o impacto do termo de regularização da descida de gradiente será\n",
    "investigado. O objetivo é utilizar a geração de atributos polinomiais de um determinado grau onde\n",
    "ocorra, claramente, uma situação de overfitting. Através da variação do peso da regularização, deve-se\n",
    "investigar qual o valor ideal deste parâmetro. Isto deve ser feito através da análise da curva de\n",
    "validação para este parâmetro. O objetivo final deste experimento é verificar se a utilização de\n",
    "regularização de um modelo complexo (que apresenta forte overfitting sem o uso da regularização)\n",
    "apresenta desempenho melhor do que a calibração do grau dos atributos polinomiais (experimento\n",
    "anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 18\n",
    "\n",
    "X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "X_ =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "#X_ =  DTS.dataset_scaling(X)\n",
    "\n",
    "lr = 10\n",
    "e = 1000\n",
    "Lambda = [0, 0.0005, 0.001, 0.005]\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "fold = 1\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "for train,val in MS.k_fold(X, k=5, shuffle=True):\n",
    "    errors[str(fold)] = []\n",
    "    val_errors[str(fold)] = []\n",
    "    loss[str(fold)] = []\n",
    "    \n",
    "    for l in Lambda:\n",
    "        \n",
    "        \n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y[train], X_[val], Y[val], epochs=e, learning_rate=lr, Lambda=l, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "    fold += 1\n",
    "        \n",
    "train_error = []        \n",
    "val_error = []\n",
    "loss_ = []\n",
    "for k in ['1', '2', '3', '4','5']:\n",
    "    train_error.append(errors[k])\n",
    "    val_error.append(val_errors[k])\n",
    "    loss_.append(loss[k])\n",
    "\n",
    "train_error = np.array(train_error)\n",
    "val_error = np.array(val_error)\n",
    "    \n",
    "train_mean, train_std = train_error.mean(axis=0), train_error.std(axis=0)\n",
    "\n",
    "val_mean, val_std = val_error.mean(axis=0), val_error.std(axis=0)\n",
    "\n",
    "print train_mean\n",
    "print train_std\n",
    "print\n",
    "print val_mean\n",
    "print val_std\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Lambda, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(Lambda, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(Lambda, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(Lambda, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "#ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "title = u'Curva de aprendizado média com #epocas = ' + str(e) + ', $\\lambda$ = ' + str(Lambda) + ' e $\\alpha$ = ' + str(lr)\n",
    "#plt.ylim([0.1, 1.])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(Lambda)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "#plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_lambda.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de aprendizado\n",
    "\n",
    "Neste experimento, deve-se utilizar a curva de aprendizado para analisar o melhor modelo\n",
    "encontrado nos experimentos anteriores (grau de polinômio e peso da regularização). A curva gerada\n",
    "deve ser analisada para depurar o modelo e concluir quais atitudes podem ser tomadas para melhorar o\n",
    "desempenho do classificador, se for necessário. A curva gerada deve conter, pelo menos, 5 pontos (5\n",
    "tamanhos diferentes do conjunto de treino)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** atributos polinomiais **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEJCAYAAABlmAtYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b43736310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 2\n",
    "\n",
    "X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "X_norm =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "#X_norm =  DTS.dataset_scaling(X)\n",
    "\n",
    "size = X.shape[0]\n",
    "\n",
    "M = [int(0.6*size), int(0.7*size), int(0.8*size), int(0.9*size), int(0.95*size), int(size)]\n",
    "\n",
    "lr = 10\n",
    "e = 1000\n",
    "Lambda = 0\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "indices = range(size)\n",
    "\n",
    "train_mean = []\n",
    "train_std = []\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "for m in M:\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = np.array(X_norm[indices[0:m]])\n",
    "    Y_ = np.array(Y[indices[0:m]])\n",
    "    fold = 1\n",
    "    for train,val in MS.k_fold(X_, k=5, shuffle=True):\n",
    "        errors[str(fold)] = []\n",
    "        val_errors[str(fold)] = []\n",
    "        loss[str(fold)] = []\n",
    "\n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y_[train], X_[val], Y_[val], epochs=e, learning_rate=lr, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "        fold += 1\n",
    "        \n",
    "    train_error = []        \n",
    "    val_error = []\n",
    "    loss_ = []\n",
    "    for k in ['1', '2', '3', '4','5']:\n",
    "        train_error.append(errors[k])\n",
    "        val_error.append(val_errors[k])\n",
    "        loss_.append(loss[k])\n",
    "\n",
    "    train_error = np.array(train_error)\n",
    "    val_error = np.array(val_error)\n",
    "\n",
    "    train_mean.append(train_error.mean(axis=0)[0])\n",
    "    train_std.append(train_error.std(axis=0)[0])\n",
    "\n",
    "    val_mean.append(val_error.mean(axis=0)[0])\n",
    "    val_std.append(val_error.std(axis=0)[0])\n",
    "\n",
    "    #print train_mean\n",
    "    #print train_std\n",
    "    #print\n",
    "    #print val_mean\n",
    "    #print val_std\n",
    "\n",
    "train_mean = np.array(train_mean)\n",
    "train_std = np.array(train_std)\n",
    "val_mean = np.array(val_mean)\n",
    "val_std = np.array(val_std)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(M, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(M, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(M, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(M, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "#ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "title = u'Curva de aprendizado média com #epocas = ' + str(e) + ', $\\lambda$ = ' + str(Lambda) + ' e $\\alpha$ = ' + str(lr)\n",
    "#plt.ylim([0.1, 1.])\n",
    "plt.xlabel('M')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(M)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "#plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_M_reg.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** regularização **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RLR = RegularizedLogisticRegression()\n",
    "MS = ModelSelection()\n",
    "DTS = DATASET()\n",
    "PLT = Plot()\n",
    "\n",
    "# carrega dataset\n",
    "X,Y = RLR.load_dataset('datasets/mnist.csv', header=True)\n",
    "\n",
    "degree = 18\n",
    "\n",
    "X_pol = DTS.generate_polynomial_attributes(X, degree)\n",
    "# normaliza\n",
    "X_norm =  DTS.dataset_scaling(X_pol)\n",
    "\n",
    "# normaliza\n",
    "#X_norm =  DTS.dataset_scaling(X)\n",
    "\n",
    "size = X.shape[0]\n",
    "\n",
    "M = [int(0.6*size), int(0.7*size), int(0.8*size), int(0.9*size), int(0.95*size), int(size)]\n",
    "\n",
    "lr = 10\n",
    "e = 1000\n",
    "Lambda = 0.005\n",
    "colors = ['red', 'cyan', 'black', 'yellow', 'green', 'gray', 'darkblue']\n",
    "\n",
    "errors = {}\n",
    "val_errors = {}\n",
    "loss = {}\n",
    "\n",
    "indices = range(size)\n",
    "\n",
    "train_mean = []\n",
    "train_std = []\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "for m in M:\n",
    "    np.random.shuffle(indices)\n",
    "    X_ = np.array(X_norm[indices[0:m]])\n",
    "    Y_ = np.array(Y[indices[0:m]])\n",
    "    fold = 1\n",
    "    for train,val in MS.k_fold(X_, k=5, shuffle=True):\n",
    "        errors[str(fold)] = []\n",
    "        val_errors[str(fold)] = []\n",
    "        loss[str(fold)] = []\n",
    "\n",
    "        # ajusta o modelo\n",
    "        RLR.fit(X_[train], Y_[train], X_[val], Y_[val], epochs=e, learning_rate=lr, Lambda=Lambda, print_results=False)\n",
    "\n",
    "        # calcula o erro no treino\n",
    "        errors[str(fold)].append(RLR.train_error[-1])\n",
    "        # calcula o erro no teste\n",
    "        val_errors[str(fold)].append(RLR.val_error[-1])\n",
    "        # salva loss\n",
    "        loss[str(fold)].append(RLR.loss)\n",
    "\n",
    "        fold += 1\n",
    "        \n",
    "    train_error = []        \n",
    "    val_error = []\n",
    "    loss_ = []\n",
    "    for k in ['1', '2', '3', '4','5']:\n",
    "        train_error.append(errors[k])\n",
    "        val_error.append(val_errors[k])\n",
    "        loss_.append(loss[k])\n",
    "\n",
    "    train_error = np.array(train_error)\n",
    "    val_error = np.array(val_error)\n",
    "\n",
    "    train_mean.append(train_error.mean(axis=0)[0])\n",
    "    train_std.append(train_error.std(axis=0)[0])\n",
    "\n",
    "    val_mean.append(val_error.mean(axis=0)[0])\n",
    "    val_std.append(val_error.std(axis=0)[0])\n",
    "\n",
    "    #print train_mean\n",
    "    #print train_std\n",
    "    #print\n",
    "    #print val_mean\n",
    "    #print val_std\n",
    "\n",
    "train_mean = np.array(train_mean)\n",
    "train_std = np.array(train_std)\n",
    "val_mean = np.array(val_mean)\n",
    "val_std = np.array(val_std)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(M, train_mean, color='red', linewidth=1)\n",
    "ax.fill_between(M, train_mean-train_std, train_mean+train_std , color='red', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.plot(M, val_mean, color='blue', linewidth=1)\n",
    "ax.fill_between(M, val_mean-val_std, val_mean+val_std , color='blue', linewidth=1, alpha=0.2)\n",
    "#ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "legends = [mpatches.Patch(color='red', label='treino'), mpatches.Patch(color='blue', label=u'validação')]\n",
    "\n",
    "title = u'Curva de aprendizado média com #epocas = ' + str(e) + ', $\\lambda$ = ' + str(Lambda) + ' e $\\alpha$ = ' + str(lr)\n",
    "#plt.ylim([0.1, 1.])\n",
    "plt.xlabel('M')\n",
    "plt.ylabel(u'Erro Binário Médio')\n",
    "plt.xticks(M)\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "box.width, box.height * 0.9])\n",
    "lgd = ax.legend(handles=legends[:], loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
    "#plt.title(title)\n",
    "plt.grid(True)\n",
    "\n",
    "fig.savefig('curva_ap_M_reg.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
